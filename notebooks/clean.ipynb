{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import qa_functions as qa\n",
    "import utils\n",
    "import temperature_data_fns as td\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cleaning_fns as clf\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file format for the raw data you have, either parquet or csv\n",
    "file_format = qa.set_file_format(file_format=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which folder contains your local directory\n",
    "# eoh_folder = os.environ.get(\"EoH\")\n",
    "eoh_folder = os.environ.get(\"EoH\")\n",
    "\n",
    "location, location_in_raw, location_out, location_out_cleaned = utils.create_folder_structure(eoh_folder)\n",
    "\n",
    "# Generate a list of all files to be run through the code\n",
    "files_info=[]\n",
    "\n",
    "for home in os.listdir(location_in_raw):\n",
    "    size = os.stat(os.path.join(location_in_raw, home))[6]\n",
    "    file_info = [home, round(size/1024)]\n",
    "    files_info.append(file_info)\n",
    "# Remove properties which do not have more than 1KB of data\n",
    "files_info = pd.DataFrame(files_info, columns=[\"Property_Name\", \"Size_KB\"])\n",
    "files_info = files_info[files_info[\"Size_KB\"] > 1]\n",
    "files_list = list(files_info.iloc[:, 0])\n",
    "files_list = [file[0:7] for file in files_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the performance factor ranges for different time scales\n",
    "# For short time scales, we expect higher variation in the performance factor\n",
    "# short time periods are around a day, long are around a year, medium is in between these two\n",
    "spf_ranges = {\n",
    "    \"short\": {\"min\": 0.75, \"max\": 7.5},\n",
    "    \"medium\": {\"min\": 0.9, \"max\": 6.5},\n",
    "    \"long\": {\"min\": 1.5, \"max\": 5.0},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is commented out as it takes some time to run\n",
    "# You can run the code in this cell to generate the temperature_stats.csv\n",
    "# Alternately if you have already generated the csv then\n",
    "# you can load the csv in the following cell to view the plots below\n",
    "\n",
    "all_stats = pd.DataFrame()\n",
    "for i, file in zip(tqdm(range(len(files_list))), files_list):\n",
    "    id = file[0:9]\n",
    "    temperature_data = utils.load_temperature_data(os.path.join(location_in_raw, id), file_format=file_format)\n",
    "    temperature_data = qa.round_timestamps(temperature_data, n_mins=2)\n",
    "    path = os.path.join(location_out, \"plots\", \"temperature\", id + \".png\")\n",
    "    td.plot_data( temperature_data, path=path )\n",
    "    temperature_data = temperature_data.reset_index()\n",
    "    stats = td.get_temperature_stats(temperature_data, id)\n",
    "    all_stats = pd.concat([all_stats, stats])\n",
    "\n",
    "path = os.path.join(location_out, \"temperature_stats.csv\")\n",
    "all_stats.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the temperature_stats.csv file, we create classifiers and make predictions\n",
    "# and create the homes_dropped subset of homes\n",
    "(\n",
    "    classifier_A,\n",
    "    classifier_B,\n",
    "    predictions_A,\n",
    "    predictions_B,\n",
    "    stats_to_use,\n",
    "    all_stats,\n",
    "    homes_dropped,\n",
    ") = clf.classify_and_predict(location_out, files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell generates 3 more subsets of homes:\n",
    "# unflagged homes, single_flagged_homes, double_flagged homes\n",
    "\n",
    "output = stats_to_use.copy()\n",
    "output[\"prediction_A\"] = predictions_A\n",
    "output[\"prediction_B\"] = predictions_B\n",
    "output[\"flagged_A\"] = output[\"prediction_A\"] != (output[\"sensor_type\"] == \"Hot_Water_Flow_Temperature\")\n",
    "output[\"flagged_B\"] = output[\"prediction_B\"] != (output[\"sensor_type\"] == \"Hot_Water_Flow_Temperature\")\n",
    "\n",
    "# There is a fair difference between how the splits behave, we will have a look at all of them\n",
    "output[\"flagged\"] = output[\"flagged_A\"] | output[\"flagged_B\"]\n",
    "\n",
    "home_flags = output.groupby([\"Property_ID\"])[\"flagged\"].sum()\n",
    "output = pd.merge(output, home_flags, on=\"Property_ID\", how=\"left\", suffixes=[None, \"_per_home\"])\n",
    "\n",
    "# Merge in all_stats to get the issue for the homes dropped for having < 50% temp data\n",
    "output = pd.merge(\n",
    "    output,\n",
    "    all_stats[[\"sensor_type\", \"Property_ID\", \"issue\"]],\n",
    "    on=[\"sensor_type\", \"Property_ID\"],\n",
    "    how=\"outer\",\n",
    ")\n",
    "\n",
    "flagged_homes = output[output[\"flagged_per_home\"] > 0]\n",
    "flagged_homes = flagged_homes.groupby(\"Property_ID\").agg({\"flagged_per_home\": \"max\"})\n",
    "unflagged_homes = np.sort(output.loc[output[\"flagged_per_home\"] == 0][\"Property_ID\"].unique())\n",
    "single_flagged_homes = flagged_homes.loc[flagged_homes[\"flagged_per_home\"] == 1]\n",
    "double_flagged_homes = flagged_homes.loc[flagged_homes[\"flagged_per_home\"] == 2]\n",
    "single_flagged_homes = single_flagged_homes.index.sort_values()\n",
    "double_flagged_homes = double_flagged_homes.index.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is where cleaning for each home takes\n",
    "\n",
    "home_classifications = {\n",
    "    \"unflagged\": unflagged_homes,\n",
    "    \"single_flagged\": single_flagged_homes,\n",
    "    \"double_flagged\": double_flagged_homes,\n",
    "    \"dropped\": homes_dropped,\n",
    "}\n",
    "\n",
    "# Loop through each home\n",
    "all_dropped_dupes = 0\n",
    "window_method = \"best\"\n",
    "homes_with_error = []\n",
    "home_summary = []\n",
    "all_windows = pd.DataFrame()\n",
    "all_homes_alteration_record = pd.DataFrame()\n",
    "alteration_record = pd.DataFrame()\n",
    "\n",
    "for home_classification in [\"unflagged\", \"single_flagged\", \"double_flagged\", \"dropped\"]:\n",
    "    for i, home in zip(\n",
    "        tqdm(range(len(home_classifications.get(home_classification)))), home_classifications.get(home_classification)\n",
    "    ):\n",
    "        temp_cleaning_type = home_classification\n",
    "\n",
    "        home_summary_part = clf.cleaning(\n",
    "            home,\n",
    "            location,\n",
    "            location_in_raw,\n",
    "            location_out_cleaned,\n",
    "            spf_ranges,\n",
    "            all_dropped_dupes,\n",
    "            alteration_record,\n",
    "            all_homes_alteration_record,\n",
    "            temp_cleaning_type,\n",
    "            output,\n",
    "            classifier_A,\n",
    "            classifier_B,\n",
    "            file_format,\n",
    "        )\n",
    "\n",
    "        home_summary.append(home_summary_part)\n",
    "\n",
    "home_summary = pd.concat(home_summary)\n",
    "home_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the outcomes output\n",
    "output = output.sort_values(\"Property_ID\")\n",
    "output.to_csv(os.path.join(location_out, \"temperature_stats_with_outcome.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save home summary\n",
    "home_summary.to_csv(os.path.join(location_out, \"home_summary_partial_1.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total dropped duplicates: {all_dropped_dupes}\")\n",
    "\n",
    "# Save out the alteration record for all the homes\n",
    "all_homes_alteration_record.to_csv(os.path.join(location_out, \"summary_of_alterations.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EoH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75e2ae2ee40dc0b50bede531bb7484755e9327c86a016df756038dbd1defadd2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
